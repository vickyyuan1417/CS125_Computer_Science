{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "Copy of EE435_PyTorch_Assignment.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1bc4af680f6542c8bcc3c9d34f3d7ab9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_a95f33dab42f4f2db0928a3579134dce",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_179663bedf7e4b5aa5153c0e24cdebdc",
              "IPY_MODEL_4635c25cfaf34ef1875b21f9ed499540",
              "IPY_MODEL_997a99e2f0e2479caaf5f8d128297fa1"
            ]
          }
        },
        "a95f33dab42f4f2db0928a3579134dce": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "179663bedf7e4b5aa5153c0e24cdebdc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_2e75f0cd4dfb4ad0aa96ac42242022ba",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6c69e32312af478483dbc6ed79790782"
          }
        },
        "4635c25cfaf34ef1875b21f9ed499540": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_2c83be22aacd421d92507e78d21617e8",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 170498071,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 170498071,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_18d6291a313b41a2a03613e5641d367d"
          }
        },
        "997a99e2f0e2479caaf5f8d128297fa1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_21e51efc982c40e395ba572e19ceb847",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 170499072/? [00:02&lt;00:00, 63126867.77it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0542769138074c8faab12bfa5a7727c6"
          }
        },
        "2e75f0cd4dfb4ad0aa96ac42242022ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6c69e32312af478483dbc6ed79790782": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2c83be22aacd421d92507e78d21617e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "18d6291a313b41a2a03613e5641d367d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "21e51efc982c40e395ba572e19ceb847": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0542769138074c8faab12bfa5a7727c6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vickyyuan1417/Lab11-Web/blob/master/Copy_of_EE435_PyTorch_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tOJS4unEuwiJ"
      },
      "source": [
        "# Coding CNNs from Scratch with Pytorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iAglL7KZu3ge"
      },
      "source": [
        "In this assignment you will code a famous CNN architecture AlexNet (https://papers.nips.cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html) to classify images from the CIFAR10 dataset (https://www.cs.toronto.edu/~kriz/cifar.html), which consists of 10 classes of natural images such as vehicles or animals. AlexNet is a landmark architecture because it was one of the first extremely deep CNNs trained on GPUs, and achieved state-of-the-art performance in the ImageNet challenge in 2012. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nv77OEtlxuP8"
      },
      "source": [
        "A lot of code will already be written to familiarize yourself with PyTorch, but you will have to fill in parts that will apply your knowledge of CNNs. Additionally, there are some numbered questions that you must answer either in a separate document, or in this notebook. Some questions may require you to do a little research. To type in the notebook, you can insert a text cell. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dr5aNOagwwm5"
      },
      "source": [
        "Let's start by installing PyTorch and the torchvision package below. Due to the size of the network, you will have to run on a GPU. So, click on the Runtime dropdown, then Change Runtime Type, then GPU for the hardware accelerator. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HXnfRg4IulGd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "d7b8eed4-fb46-4487-d904-89e29b53b35e"
      },
      "source": [
        "!pip install pytorch\n",
        "!pip install torchvision"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytorch\n",
            "  Using cached pytorch-1.0.2.tar.gz (689 bytes)\n",
            "Building wheels for collected packages: pytorch\n",
            "  Building wheel for pytorch (setup.py) ... \u001b[?25lerror\n",
            "\u001b[31m  ERROR: Failed building wheel for pytorch\u001b[0m\n",
            "\u001b[?25h  Running setup.py clean for pytorch\n",
            "Failed to build pytorch\n",
            "Installing collected packages: pytorch\n",
            "    Running setup.py install for pytorch ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[31mERROR: Command errored out with exit status 1: /usr/bin/python3 -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-install-jhot17v_/pytorch_a0a1c57f16d143cf82edbc5aa1310c1b/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-install-jhot17v_/pytorch_a0a1c57f16d143cf82edbc5aa1310c1b/setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record /tmp/pip-record-dxdv49h0/install-record.txt --single-version-externally-managed --compile --install-headers /usr/local/include/python3.7/pytorch Check the logs for full command output.\u001b[0m\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (0.11.1+cu111)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision) (7.1.2)\n",
            "Requirement already satisfied: torch==1.10.0 in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.10.0+cu111)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.21.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.10.0->torchvision) (3.10.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VtC0KJcdufBE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "7e11b2b9-a19f-4cbd-aab9-aab9e5f29ece"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "device = torch.device('cuda')\n",
        "    \n",
        "print(device)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5DML-S0AX-_o"
      },
      "source": [
        "### 1. In the following cell, we are employing something called \"data augmentation\" with random horizontal and vertical flips. So when training data is fed into the network, it is ranadomly transformed. What are advantages of this?\n",
        "#### Ans: We do \"data augmentation\" to improve our model perforance by forming new and randomly different examples to train datasets. \n",
        "\n",
        "### 2. We normalize with the line transforms.Normalize((0.5,), (0.5,)). What are the benefits of normalizing data? \n",
        "#### Ans: The Benefits is to help get data within a range and reduces the skewness which helps learn faster and better.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eruiC4sAufBL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136,
          "referenced_widgets": [
            "1bc4af680f6542c8bcc3c9d34f3d7ab9",
            "a95f33dab42f4f2db0928a3579134dce",
            "179663bedf7e4b5aa5153c0e24cdebdc",
            "4635c25cfaf34ef1875b21f9ed499540",
            "997a99e2f0e2479caaf5f8d128297fa1",
            "2e75f0cd4dfb4ad0aa96ac42242022ba",
            "6c69e32312af478483dbc6ed79790782",
            "2c83be22aacd421d92507e78d21617e8",
            "18d6291a313b41a2a03613e5641d367d",
            "21e51efc982c40e395ba572e19ceb847",
            "0542769138074c8faab12bfa5a7727c6"
          ]
        },
        "outputId": "1bc48506-658c-4af1-9e3b-59b8d783fefc"
      },
      "source": [
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import random_split\n",
        "from math import ceil\n",
        "\n",
        "BATCH_SIZE = 100\n",
        "\n",
        "\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.RandomHorizontalFlip(p=0.5), \n",
        "     transforms.RandomVerticalFlip(p=0.5),\n",
        "     transforms.Normalize((0.5,), (0.5,))])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "\n",
        "torch.manual_seed(43)\n",
        "val_size = 10000\n",
        "train_size = len(trainset) - val_size\n",
        "\n",
        "\n",
        "train_ds, val_ds = random_split(trainset, [train_size, val_size])\n",
        "print(len(train_ds), len(val_ds))\n",
        "\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "num_steps =  ceil(len(train_ds) / BATCH_SIZE)\n",
        "num_steps"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1bc4af680f6542c8bcc3c9d34f3d7ab9",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/170498071 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n",
            "40000 10000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "400"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NLzuKuJxufBM"
      },
      "source": [
        "train_loader = torch.utils.data.DataLoader(train_ds, BATCH_SIZE, shuffle=True, drop_last = True)\n",
        "val_loader = torch.utils.data.DataLoader(val_ds, BATCH_SIZE)\n",
        "test_loader = torch.utils.data.DataLoader(testset, BATCH_SIZE)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7rT3aDd7aVLm"
      },
      "source": [
        "You can insert an integer  into the code trainset[#insert integer] to visualize images from the training set. Some of the images might look weird because they have been randomly flipped according to our data augmentation scheme. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        },
        "id": "wV-W2b6eZaoG",
        "outputId": "2fa93e75-ea65-40b3-bc6c-7ab5a840d52d"
      },
      "source": [
        "img, label = trainset[1]\n",
        "plt.imshow((img.permute((1, 2, 0))+1)/2)\n",
        "print('Label (numeric):', label)\n",
        "print('Label (textual):', classes[label])"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label (numeric): 9\n",
            "Label (textual): truck\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wVLQM7ODamEm"
      },
      "source": [
        "Now comes the fun part. You will have to put in the correct parameters into different torch.nn functions in order to convolve and downsample the image into the correct dimensionality for classification. Think of it as a puzzle. You will insert the parameters where there is a comment #TODO. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ohY_5zoBufBN"
      },
      "source": [
        "class Discriminator(torch.nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(in_channels = 3, #TODO,\n",
        "                      out_channels = 64, #TODO,\n",
        "                      kernel_size=3, stride=2, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2),\n",
        "            nn.Conv2d(64, 192, kernel_size=3, \n",
        "                      padding= 2 ), #TODO),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2),\n",
        "            nn.Conv2d( 192, #TODO, \n",
        "                       384, #TODO,\n",
        "                      kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2),\n",
        "        )\n",
        "            \n",
        "        #Fully connected layers\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(),\n",
        "            nn.Linear( 1024, 4096), #TODO 256 * 6 * 6),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(4096, 4096 ),#TODO),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(4096, 10), #TODO),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        #we must flatten our feature maps before feeding into fully connected layers\n",
        "        x = x.contiguous().view(x.size(0),1024) #TODO 256 * 6 * 6)\n",
        "        x = self.classifier(x)\n",
        "        return x"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bw2ZfQfAeZum"
      },
      "source": [
        "Below we are initializing our model with a weight scheme."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AlUFRnhZufBN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "905f46ff-43b7-44f4-f4af-e3f6a051b4e5"
      },
      "source": [
        "net = Discriminator()\n",
        "\n",
        "def weights_init(m):\n",
        "\n",
        "    classname = m.__class__.__name__\n",
        "\n",
        "    if classname.find('Conv') != -1:\n",
        "        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
        "\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
        "        torch.nn.init.constant_(m.bias.data, 0)\n",
        "\n",
        "\n",
        "# Initialize Models\n",
        "net = net.to(device)\n",
        "\n",
        "net.apply(weights_init)\n",
        "\n"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Discriminator(\n",
              "  (features): Sequential(\n",
              "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (3): Conv2d(64, 192, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n",
              "    (4): ReLU(inplace=True)\n",
              "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (7): ReLU(inplace=True)\n",
              "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (9): ReLU(inplace=True)\n",
              "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (11): ReLU(inplace=True)\n",
              "    (12): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (classifier): Sequential(\n",
              "    (0): Dropout(p=0.5, inplace=False)\n",
              "    (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "    (2): ReLU(inplace=True)\n",
              "    (3): Dropout(p=0.5, inplace=False)\n",
              "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "    (5): ReLU(inplace=True)\n",
              "    (6): Linear(in_features=4096, out_features=10, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tFBRnApFfZUr"
      },
      "source": [
        "# 3. Notice above in our network architecture, we have what are called \"Dropout\" layers. What is the point of these?\n",
        "#### Ans: Dropout layer is to prevent our Alexnet from overfitting, it randomly nullifies some features with a frequency of rate at each step during training time."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IqzRwTN-febi"
      },
      "source": [
        "Defining our cost/loss function, which is cross-entropy loss. We also define our optimizer with hyperparameters: learning rate and betas. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sNvPJc_PufBN"
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(\n",
        "    net.parameters(),\n",
        "    lr=0.0005,\n",
        "    betas = (0.5, 0.999)\n",
        ")\n"
      ],
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JR6xLm7KiR3p"
      },
      "source": [
        "Below we actually train our network. Run for just 10 epochs. It takes some time. Wherever there is the comment #TODO, you must insert code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2VwEjNs3ufBO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "5764591c-5eb7-4154-eda9-f45338e17caa"
      },
      "source": [
        "from torch._C import Size\n",
        "for epoch in range(15):  # loop over the dataset multiple times\n",
        "    # net.train()\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(train_loader, 0):\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        inputs, labels = data\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device) #TODO\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "        # forward + backward + optimize\n",
        "        outputs = net(inputs) #TODO     #pass input data into network to get outputs\n",
        "        loss = criterion(outputs, labels) #TODO)\n",
        "        loss.backward()  #calculate gradients\n",
        "        optimizer.step() #take gradient descent step\n",
        "\n",
        "        \n",
        "        running_loss += loss.item()\n",
        "            \n",
        "    \n",
        "    print(\"E:{}, Train Loss:{}\".format(\n",
        "                epoch+1,\n",
        "                running_loss / num_steps\n",
        "            )\n",
        "        )\n",
        "        \n",
        "        \n",
        "        \n",
        "    #validation\n",
        "    # net.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for data in val_loader:\n",
        "            #TODO: load images and labels from validation loader\n",
        "            images,labels = data\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = net(images)  #TODO  #run forward pass\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "                \n",
        "            loss = criterion(outputs, labels)    #TODO       #calculate validation loss\n",
        "            val_loss += loss.item()\n",
        "    val_loss /=num_steps\n",
        "    print('Accuracy of 10000 val images: {}'.format( correct / total))\n",
        "    print('Val Loss: {}'.format( val_loss))\n",
        "print('Finished Training')"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E:1, Train Loss:0.28755450589582326\n",
            "Accuracy of 10000 val images: 0.7039\n",
            "Val Loss: 0.30720687463879587\n",
            "E:2, Train Loss:0.28394541922956706\n",
            "Accuracy of 10000 val images: 0.7092\n",
            "Val Loss: 0.2999604432284832\n",
            "E:3, Train Loss:0.2628721898794174\n",
            "Accuracy of 10000 val images: 0.7164\n",
            "Val Loss: 0.30306818887591364\n",
            "E:4, Train Loss:0.2654457014426589\n",
            "Accuracy of 10000 val images: 0.7111\n",
            "Val Loss: 0.2995715522766113\n",
            "E:5, Train Loss:0.26098587838932874\n",
            "Accuracy of 10000 val images: 0.7105\n",
            "Val Loss: 0.3163650222122669\n",
            "E:6, Train Loss:0.25220268044620753\n",
            "Accuracy of 10000 val images: 0.7102\n",
            "Val Loss: 0.32316431745886803\n",
            "E:7, Train Loss:0.252556214928627\n",
            "Accuracy of 10000 val images: 0.7086\n",
            "Val Loss: 0.33644179597496987\n",
            "E:8, Train Loss:0.22912811253219842\n",
            "Accuracy of 10000 val images: 0.7061\n",
            "Val Loss: 0.33479794919490813\n",
            "E:9, Train Loss:0.41534125519916415\n",
            "Accuracy of 10000 val images: 0.7175\n",
            "Val Loss: 0.28086990922689437\n",
            "E:10, Train Loss:0.3152038820460439\n",
            "Accuracy of 10000 val images: 0.7106\n",
            "Val Loss: 0.3128448170423508\n",
            "E:11, Train Loss:0.23602804033085703\n",
            "Accuracy of 10000 val images: 0.713\n",
            "Val Loss: 0.3023627215623856\n",
            "E:12, Train Loss:0.2971811249293387\n",
            "Accuracy of 10000 val images: 0.7094\n",
            "Val Loss: 0.3233750277757645\n",
            "E:13, Train Loss:0.21812742980197072\n",
            "Accuracy of 10000 val images: 0.7226\n",
            "Val Loss: 0.30969432622194293\n",
            "E:14, Train Loss:0.20664621943607928\n",
            "Accuracy of 10000 val images: 0.7096\n",
            "Val Loss: 0.3423467490077019\n",
            "E:15, Train Loss:0.19755777908489108\n",
            "Accuracy of 10000 val images: 0.7175\n",
            "Val Loss: 0.3387778890132904\n",
            "Finished Training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9sKf8Lu-mGYd"
      },
      "source": [
        "## 4. If we train for more epochs, our accuracy/performance will increase. What happens if we train for too long though? What method can be employed to mitigate this?\n",
        "#### Ans: As we increase number of Epochs, the accuracy/perforance will go high and reach a top accuracy/perforance and then go down. This is because lack of train data or model is too complex with millions of parameters. To solve this, we can add l1 or l2 regularizers, or plot the training and validation loss and keeping an eye on how they progress over epochs.\n",
        "\n",
        "## 5. Try increasing learning rate and look at the metrics for training and validation data? What do you notice? Why do think this is happening?\n",
        "#### Ans:By increase learning rate within 10 epochs, the accuracy/perforance increases significantly, which means the model is adopted to our problem much more quickly. This is because smaller learning rates require more training epochs given the smaller changes made to the weights each update, whereas larger learning rates result in rapid changes and require fewer training epochs.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lz31Vm2XmM7p"
      },
      "source": [
        "We can see the performance on the testing set now. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SNLMA4oIufBO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "c0d3bf13-1d78-4210-99fb-265033fb0445"
      },
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data in test_loader:\n",
        "        images, labels = data\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = net(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print('Accuracy of 10000 test images: {}'.format( correct / total))"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of 10000 test images: 0.719\n"
          ]
        }
      ]
    }
  ]
}